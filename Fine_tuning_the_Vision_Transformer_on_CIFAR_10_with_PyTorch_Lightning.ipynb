{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb#scrollTo=yLCEbNfcbhk6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers datasets pytorch-lightning Pillow \n",
    "%pip install -q torch torchvision torchaudio tensorboard\n",
    "%pip install -q numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a portion of cifar-10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 9.98k/9.98k [00:00<00:00, 7.03MB/s]\n",
      "Downloading data: 100%|██████████| 119M/119M [00:04<00:00, 25.7MB/s]\n",
      "Downloading data: 100%|██████████| 23.8M/23.8M [00:02<00:00, 11.0MB/s]\n",
      "Generating train split: 100%|██████████| 50000/50000 [00:01<00:00, 47232.12 examples/s]\n",
      "Generating test split: 100%|██████████| 10000/10000 [00:00<00:00, 48863.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load cifar10 (only small portion for demonstration purposes) \n",
    "train_ds, test_ds = load_dataset('cifar100', split=['train[:]', 'test[:]'])\n",
    "# split up training into training + validation\n",
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'fine_label', 'coarse_label'],\n",
       "    num_rows: 45000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDz+O3jTU5IrpifsxCkHjODgirmpMILWG8tt6q8hjYdjgDp+Fa/idbLSmaS0s7t7oOTPPKgCEnnitzwjcWup+Epbi6tY3kRpFBbtRGmpu1jKVoK7PPMpLqUSwINrsuWJzivZPDNjHNoti05+T+1RDIc4yrJjH4nFeXWNzaT30Pm2yRKrjv1OD/9avSU1G0sPCf2fzUxcXaXNtMj8bldeB6ng1FaSehtRpSaujgL+eTUI7iJ587vlCk9K7nwR4Tu4fDKjMbx3O9iQfu54xXk0mvQiQN5LQyDlgo4P1ro9G+JNxaztEZUgt4ovLhwhy3fn9aU5VFP3NCo0qcoe+x2ueH7rR/sssqx+QXClk5JOa62/wBMguvCOnwh3jVDNOh2cSpjP4dx6155qWvX+sWTYCM4YupUEcZ4P49a6ZNY36Pp0d9LFJ5FiyhS21sls4b3OKqk6lvfYTjCMvcd0f/Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_ds[0]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple': 0,\n",
       " 'aquarium_fish': 1,\n",
       " 'baby': 2,\n",
       " 'bear': 3,\n",
       " 'beaver': 4,\n",
       " 'bed': 5,\n",
       " 'bee': 6,\n",
       " 'beetle': 7,\n",
       " 'bicycle': 8,\n",
       " 'bottle': 9,\n",
       " 'bowl': 10,\n",
       " 'boy': 11,\n",
       " 'bridge': 12,\n",
       " 'bus': 13,\n",
       " 'butterfly': 14,\n",
       " 'camel': 15,\n",
       " 'can': 16,\n",
       " 'castle': 17,\n",
       " 'caterpillar': 18,\n",
       " 'cattle': 19,\n",
       " 'chair': 20,\n",
       " 'chimpanzee': 21,\n",
       " 'clock': 22,\n",
       " 'cloud': 23,\n",
       " 'cockroach': 24,\n",
       " 'couch': 25,\n",
       " 'cra': 26,\n",
       " 'crocodile': 27,\n",
       " 'cup': 28,\n",
       " 'dinosaur': 29,\n",
       " 'dolphin': 30,\n",
       " 'elephant': 31,\n",
       " 'flatfish': 32,\n",
       " 'forest': 33,\n",
       " 'fox': 34,\n",
       " 'girl': 35,\n",
       " 'hamster': 36,\n",
       " 'house': 37,\n",
       " 'kangaroo': 38,\n",
       " 'keyboard': 39,\n",
       " 'lamp': 40,\n",
       " 'lawn_mower': 41,\n",
       " 'leopard': 42,\n",
       " 'lion': 43,\n",
       " 'lizard': 44,\n",
       " 'lobster': 45,\n",
       " 'man': 46,\n",
       " 'maple_tree': 47,\n",
       " 'motorcycle': 48,\n",
       " 'mountain': 49,\n",
       " 'mouse': 50,\n",
       " 'mushroom': 51,\n",
       " 'oak_tree': 52,\n",
       " 'orange': 53,\n",
       " 'orchid': 54,\n",
       " 'otter': 55,\n",
       " 'palm_tree': 56,\n",
       " 'pear': 57,\n",
       " 'pickup_truck': 58,\n",
       " 'pine_tree': 59,\n",
       " 'plain': 60,\n",
       " 'plate': 61,\n",
       " 'poppy': 62,\n",
       " 'porcupine': 63,\n",
       " 'possum': 64,\n",
       " 'rabbit': 65,\n",
       " 'raccoon': 66,\n",
       " 'ray': 67,\n",
       " 'road': 68,\n",
       " 'rocket': 69,\n",
       " 'rose': 70,\n",
       " 'sea': 71,\n",
       " 'seal': 72,\n",
       " 'shark': 73,\n",
       " 'shrew': 74,\n",
       " 'skunk': 75,\n",
       " 'skyscraper': 76,\n",
       " 'snail': 77,\n",
       " 'snake': 78,\n",
       " 'spider': 79,\n",
       " 'squirrel': 80,\n",
       " 'streetcar': 81,\n",
       " 'sunflower': 82,\n",
       " 'sweet_pepper': 83,\n",
       " 'table': 84,\n",
       " 'tank': 85,\n",
       " 'telephone': 86,\n",
       " 'television': 87,\n",
       " 'tiger': 88,\n",
       " 'tractor': 89,\n",
       " 'train': 90,\n",
       " 'trout': 91,\n",
       " 'tulip': 92,\n",
       " 'turtle': 93,\n",
       " 'wardrobe': 94,\n",
       " 'whale': 95,\n",
       " 'willow_tree': 96,\n",
       " 'wolf': 97,\n",
       " 'woman': 98,\n",
       " 'worm': 99}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {id:label for id, label in enumerate(train_ds.features['fine_label'].names)}\n",
    "label2id = {label:id for id, label in id2label.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'streetcar'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[train_ds[0]['fine_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "image_mean = processor.image_mean\n",
    "image_std = processor.image_std\n",
    "size = processor.size[\"height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>],\n",
       " 'fine_label': [81, 58],\n",
       " 'coarse_label': [19, 18],\n",
       " 'pixel_values': [tensor([[[-0.6314, -0.6314, -0.6314,  ...,  0.0431,  0.0431,  0.0431],\n",
       "           [-0.6314, -0.6314, -0.6314,  ...,  0.0431,  0.0431,  0.0431],\n",
       "           [-0.6314, -0.6314, -0.6314,  ...,  0.0431,  0.0431,  0.0431],\n",
       "           ...,\n",
       "           [-0.7412, -0.7412, -0.7412,  ...,  0.4353,  0.4353,  0.4353],\n",
       "           [-0.7412, -0.7412, -0.7412,  ...,  0.4353,  0.4353,  0.4353],\n",
       "           [-0.7412, -0.7412, -0.7412,  ...,  0.4353,  0.4353,  0.4353]],\n",
       "  \n",
       "          [[-0.5451, -0.5451, -0.5451,  ..., -0.0196, -0.0196, -0.0196],\n",
       "           [-0.5451, -0.5451, -0.5451,  ..., -0.0196, -0.0196, -0.0196],\n",
       "           [-0.5451, -0.5451, -0.5451,  ..., -0.0196, -0.0196, -0.0196],\n",
       "           ...,\n",
       "           [-0.7804, -0.7804, -0.7804,  ...,  0.3882,  0.3882,  0.3882],\n",
       "           [-0.7804, -0.7804, -0.7804,  ...,  0.3882,  0.3882,  0.3882],\n",
       "           [-0.7804, -0.7804, -0.7804,  ...,  0.3882,  0.3882,  0.3882]],\n",
       "  \n",
       "          [[-0.5373, -0.5373, -0.5373,  ..., -0.1216, -0.1216, -0.1216],\n",
       "           [-0.5373, -0.5373, -0.5373,  ..., -0.1216, -0.1216, -0.1216],\n",
       "           [-0.5373, -0.5373, -0.5373,  ..., -0.1216, -0.1216, -0.1216],\n",
       "           ...,\n",
       "           [-0.7961, -0.7961, -0.7961,  ...,  0.2863,  0.2863,  0.2863],\n",
       "           [-0.7961, -0.7961, -0.7961,  ...,  0.2863,  0.2863,  0.2863],\n",
       "           [-0.7961, -0.7961, -0.7961,  ...,  0.2863,  0.2863,  0.2863]]]),\n",
       "  tensor([[[-0.5529, -0.5529, -0.5529,  ..., -0.7804, -0.7804, -0.7804],\n",
       "           [-0.5529, -0.5529, -0.5529,  ..., -0.7804, -0.7804, -0.7804],\n",
       "           [-0.5529, -0.5529, -0.5529,  ..., -0.7804, -0.7804, -0.7804],\n",
       "           ...,\n",
       "           [-0.5686, -0.5686, -0.5686,  ...,  0.2471,  0.2471,  0.2471],\n",
       "           [-0.5686, -0.5686, -0.5686,  ...,  0.2471,  0.2471,  0.2471],\n",
       "           [-0.5686, -0.5686, -0.5686,  ...,  0.2471,  0.2471,  0.2471]],\n",
       "  \n",
       "          [[-0.4824, -0.4824, -0.4824,  ..., -0.7490, -0.7490, -0.7490],\n",
       "           [-0.4824, -0.4824, -0.4824,  ..., -0.7490, -0.7490, -0.7490],\n",
       "           [-0.4824, -0.4824, -0.4824,  ..., -0.7490, -0.7490, -0.7490],\n",
       "           ...,\n",
       "           [-0.5137, -0.5137, -0.5137,  ...,  0.2784,  0.2784,  0.2784],\n",
       "           [-0.5137, -0.5137, -0.5137,  ...,  0.2784,  0.2784,  0.2784],\n",
       "           [-0.5137, -0.5137, -0.5137,  ...,  0.2784,  0.2784,  0.2784]],\n",
       "  \n",
       "          [[-0.4118, -0.4118, -0.4118,  ..., -0.8196, -0.8196, -0.8196],\n",
       "           [-0.4118, -0.4118, -0.4118,  ..., -0.8196, -0.8196, -0.8196],\n",
       "           [-0.4118, -0.4118, -0.4118,  ..., -0.8196, -0.8196, -0.8196],\n",
       "           ...,\n",
       "           [-0.7569, -0.7569, -0.7569,  ...,  0.3804,  0.3804,  0.3804],\n",
       "           [-0.7569, -0.7569, -0.7569,  ...,  0.3804,  0.3804,  0.3804],\n",
       "           [-0.7569, -0.7569, -0.7569,  ...,  0.3804,  0.3804,  0.3804]]])]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"fine_label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"fine_labels\": labels}\n",
    "\n",
    "train_batch_size = 512\n",
    "eval_batch_size = 512\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, collate_fn=collate_fn, batch_size=train_batch_size)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=eval_batch_size)\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 224, 224])\n",
      "fine_labels torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert batch['pixel_values'].shape == (train_batch_size, 3, 224, 224)\n",
    "assert batch['fine_labels'].shape == (train_batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataloader))['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import ViTForImageClassification, AdamW\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViTLightningModule(pl.LightningModule):\n",
    "    def __init__(self, num_labels=100):\n",
    "        super(ViTLightningModule, self).__init__()\n",
    "        self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                              num_labels=100,\n",
    "                                                              id2label=id2label,\n",
    "                                                              label2id=label2id)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        return outputs.logits\n",
    "        \n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['fine_labels']\n",
    "        logits = self(pixel_values)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits, labels)\n",
    "        predictions = logits.argmax(-1)\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        accuracy = correct/pixel_values.shape[0]\n",
    "\n",
    "        return loss, accuracy\n",
    "      \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        self.log(\"training_accuracy\", accuracy)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss, on_epoch=True)\n",
    "        self.log(\"validation_accuracy\", accuracy, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, accuracy = self.common_step(batch, batch_idx)     \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We could make the optimizer more fancy by adding a scheduler and specifying which parameters do\n",
    "        # not require weight_decay but just using AdamW out-of-the-box works fine\n",
    "        return AdamW(self.parameters(), lr=5e-5)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir ~/CIM/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type                      | Params | Mode\n",
      "----------------------------------------------------------\n",
      "0 | vit  | ViTForImageClassification | 85.9 M | eval\n",
      "----------------------------------------------------------\n",
      "85.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "85.9 M    Total params\n",
      "343.502   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  46%|████▌     | 5168/11250 [03:32<04:10, 24.27it/s, v_num=7]     "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# for early stopping, see https://pytorch-lightning.readthedocs.io/en/1.0.0/early_stopping.html?highlight=early%20stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "model = ViTLightningModule()\n",
    "trainer = Trainer(callbacks=[EarlyStopping(monitor='validation_loss')])\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/aesop/CIM/lightning_logs/version_4/checkpoints/epoch=5-step=13500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/aesop/CIM/lightning_logs/version_4/checkpoints/epoch=5-step=13500.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1000/1000 [00:06<00:00, 144.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
